{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8defc3",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfb16a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suyog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681288b",
   "metadata": {},
   "source": [
    "pip install nltk ---> download punkt, stopwords,wordnet,average perceptent_tagger by  nltk.download('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd30ca",
   "metadata": {},
   "source": [
    "## #Download nltk data lobraries. All can be downloaded by using nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a4682",
   "metadata": {},
   "source": [
    "## 1. Tokenizzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2f2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4411d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='hi myself suyog and this is freind nihar . we both have one thing common, that is chutiya friend! This typically weired thing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f261b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=sent_tokenize(text)\n",
    "#tokenizes text in sentences........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d81648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi myself suyog and this is freind nihar .',\n",
       " 'we both have one thing common, that is chutiya friend!',\n",
       " 'This typically weired thing']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6b05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "917ecda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text in words....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f00660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'myself',\n",
       " 'suyog',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'freind',\n",
       " 'nihar',\n",
       " '.',\n",
       " 'we',\n",
       " 'both',\n",
       " 'have',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'common',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'chutiya',\n",
       " 'friend',\n",
       " '!',\n",
       " 'This',\n",
       " 'typically',\n",
       " 'weired',\n",
       " 'thing']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a7504",
   "metadata": {},
   "source": [
    "## 2. Stop word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d9aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import lbraries stopword\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3898011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stwords=set(stopwords.words('english'))\n",
    "## set is imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c82a6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## store stopwords of english words in stwords .......lik is, an , the , are (which dont have any signifiacance , meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfee7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75d4c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## store tokenized words in wt to compare laterr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "547bb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text=[word for word in wt if not word in stwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36ebe09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare and store word from wt if it doesnt exist in stwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50988775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'suyog',\n",
       " 'freind',\n",
       " 'nihar',\n",
       " '.',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'common',\n",
       " ',',\n",
       " 'chutiya',\n",
       " 'friend',\n",
       " '!',\n",
       " 'This',\n",
       " 'typically',\n",
       " 'weired',\n",
       " 'thing']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b978e",
   "metadata": {},
   "source": [
    "## 3. Stemming\n",
    "\n",
    "Stemming is a process  to reduce words to their base or root form. \n",
    "\n",
    "Goal is to normalize words so that variations of the same word are treated as the same word.\n",
    "\n",
    "For example, consider the words \"running,\" \"runs,\" and \"ran.\" These words have a common base form, which is \"run.\"\n",
    "\n",
    "Stemming algorithms work by removing prefixes, suffixes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9024da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required data/ vars\n",
    "text= 'we are taking this sentence only for sake of stemming example . As it should contain the words like loving , kicking, dumping ! '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dfa165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04246efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'are',\n",
       " 'taking',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'only',\n",
       " 'for',\n",
       " 'sake',\n",
       " 'of',\n",
       " 'stemming',\n",
       " 'example',\n",
       " '.',\n",
       " 'As',\n",
       " 'it',\n",
       " 'should',\n",
       " 'contain',\n",
       " 'the',\n",
       " 'words',\n",
       " 'like',\n",
       " 'loving',\n",
       " ',',\n",
       " 'kicking',\n",
       " ',',\n",
       " 'dumping',\n",
       " '!']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "902e95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer.\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball stemming algorithm, also known as the Porter2 algorithm, is a widely used stemming algorithm in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75cc86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=SnowballStemmer('english')\n",
    "\n",
    "# creates an instance of the SnowballStemmer class and assigns it to the variable st. \n",
    "#The argument 'english' specifies that the stemming should be performed for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "309525b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD II FOR STEMMING\n",
    "results = []\n",
    "for word in wt:\n",
    "    stemmed_word = st.stem(word)\n",
    "    if word.lower() != stemmed_word:\n",
    "        results.append((word, stemmed_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37cf3a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taking', 'take'),\n",
       " ('sentence', 'sentenc'),\n",
       " ('only', 'onli'),\n",
       " ('stemming', 'stem'),\n",
       " ('example', 'exampl'),\n",
       " ('words', 'word'),\n",
       " ('loving', 'love'),\n",
       " ('kicking', 'kick'),\n",
       " ('dumping', 'dump')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef6a17",
   "metadata": {},
   "source": [
    "##  3. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab85538",
   "metadata": {},
   "source": [
    "### stemming can often create non-existent words, whereas lemmas are actual words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4448bb9",
   "metadata": {},
   "source": [
    "process to reduce words to their base or canonical form, known as the **lemma**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f692e",
   "metadata": {},
   "source": [
    "The goal of lemmatization is to obtain the root form of a word, which can be a **valid** word itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433216e",
   "metadata": {},
   "source": [
    "the lemma of the word \"better\" as an adjective is \"good,\" while as a verb, it remains \"better.\" Lemmatization ensures that words are reduced to a consistent form based on their POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd13bc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are taking this sentence only for sake of stemming example . As it should contain the words like loving , kicking, dumping ! '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21490676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'are',\n",
       " 'taking',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'only',\n",
       " 'for',\n",
       " 'sake',\n",
       " 'of',\n",
       " 'stemming',\n",
       " 'example',\n",
       " '.',\n",
       " 'As',\n",
       " 'it',\n",
       " 'should',\n",
       " 'contain',\n",
       " 'the',\n",
       " 'words',\n",
       " 'like',\n",
       " 'loving',\n",
       " ',',\n",
       " 'kicking',\n",
       " ',',\n",
       " 'dumping',\n",
       " '!']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b45b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize stemmer\n",
    "\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a50344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## crEATE INSTANCE\n",
    "lt=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ec9109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=[]\n",
    "for word in wt:\n",
    "    lword=lt.lemmatize(word)\n",
    "    if word!=lword:\n",
    "        result2.append((word,lword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1798547f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('words', 'word')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56c9b3",
   "metadata": {},
   "source": [
    "## 4. POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262fd0cc",
   "metadata": {},
   "source": [
    "--> Process that assigns grammatical tags or labels to words in a text, based on their part of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb8088",
   "metadata": {},
   "source": [
    "POS tags represent the grammatical category of a word, such as noun, verb, adjective, adverb, pronoun, conjunction, preposition, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f66ea1d",
   "metadata": {},
   "source": [
    "NN-noun  ,VB-Verb\n",
    "JJ-Adjective  ,\n",
    "Rb-Adverb  ,\n",
    "PRP-Pronoun  ,\n",
    "DT-Determiner  ,\n",
    "IN-preposition  ,\n",
    "CC-Conjuction  ,\n",
    "UH-Interjection  ,\n",
    "CD-Numerical  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bcfeeb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfb1590a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('taking', 'VBG'),\n",
       " ('this', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('only', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('sake', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('stemming', 'VBG'),\n",
       " ('example', 'NN'),\n",
       " ('As', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('should', 'MD'),\n",
       " ('contain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('words', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('loving', 'NN'),\n",
       " ('kicking', 'NN'),\n",
       " ('dumping', 'VBG')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(text).tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f408644",
   "metadata": {},
   "source": [
    "# B. calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f99072",
   "metadata": {},
   "source": [
    "Term frequency TF(W,D) = (count of word W in doc d) / (total no of word in doc D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b1b1e",
   "metadata": {},
   "source": [
    "Inverse DF =(total no of doc 'n')  / ('WN' no of docs containing 'W')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b219918",
   "metadata": {},
   "source": [
    "TF-IDF = TF (W,D) * IDF (W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38cb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be466cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "395dba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of documents\n",
    "documents = [\n",
    "    \"Jupyter is the largest planet\",\n",
    "    \"Mars is the fourth planet from the sun\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14e245db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of TfidfVectorizer\n",
    "vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20d2452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer (vect) to the documents and transform the documents to TF-IDF representation\n",
    "tfidf_matrix = vect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7de44467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (terms)\n",
    "feature_names = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "504205a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.3793034928087496\n",
      "  (0, 4)\t0.5330978245262535\n",
      "  (0, 8)\t0.3793034928087496\n",
      "  (0, 2)\t0.3793034928087496\n",
      "  (0, 3)\t0.5330978245262535\n",
      "  (1, 7)\t0.37695708675831013\n",
      "  (1, 1)\t0.37695708675831013\n",
      "  (1, 0)\t0.37695708675831013\n",
      "  (1, 5)\t0.37695708675831013\n",
      "  (1, 6)\t0.2682080718928097\n",
      "  (1, 8)\t0.5364161437856194\n",
      "  (1, 2)\t0.2682080718928097\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af7bdc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "fourth : 0.0\n",
      "from : 0.0\n",
      "is : 0.3793034928087496\n",
      "jupyter : 0.5330978245262535\n",
      "largest : 0.5330978245262535\n",
      "mars : 0.0\n",
      "planet : 0.3793034928087496\n",
      "sun : 0.0\n",
      "the : 0.3793034928087496\n",
      "\n",
      "Document 2\n",
      "fourth : 0.37695708675831013\n",
      "from : 0.37695708675831013\n",
      "is : 0.2682080718928097\n",
      "jupyter : 0.0\n",
      "largest : 0.0\n",
      "mars : 0.37695708675831013\n",
      "planet : 0.2682080718928097\n",
      "sun : 0.37695708675831013\n",
      "the : 0.5364161437856194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the TF-IDF representation for each document\n",
    "for i in range(len(documents)):\n",
    "    print(\"Document\", i+1)\n",
    "    for j in range(len(feature_names)):\n",
    "        print(feature_names[j], \":\", tfidf_matrix[i, j])  # i,j combine in []\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28248ddd",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affc008",
   "metadata": {},
   "source": [
    "pip install nltk ---> download punkt, stopwords,wordnet,average perceptent_tagger by nltk.download('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009395a",
   "metadata": {},
   "source": [
    "tokenization == sent_tokenize , word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997fec43",
   "metadata": {},
   "source": [
    "stopwords =from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ac34f",
   "metadata": {},
   "source": [
    "Stemmer = from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1b91d",
   "metadata": {},
   "source": [
    "lemmazation ==from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079fe211",
   "metadata": {},
   "source": [
    "POS TAG = from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325feca",
   "metadata": {},
   "source": [
    "TF-IDF = from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
